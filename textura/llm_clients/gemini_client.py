import os
import google.generativeai as genai
from typing import Any # For kwargs

from .base import BaseLLMClient

# Recommended: Set up a specific logger for this client if extensive logging is needed.
# import logging
# logger = logging.getLogger(__name__)

class GeminiClient(BaseLLMClient):
    """
    An LLM client for interacting with Google's Gemini models.

    Configuration via Environment Variables:
    - GOOGLE_API_KEY: Your Google API key for Gemini. This is required for authentication.
    - TEXTURA_LLM_MODEL: Specifies the Gemini model to be used (e.g., "gemini-1.5-pro-latest").
      If not set, defaults to "gemini-1.5-pro-latest".

    Note: Google's client libraries might also implicitly pick up credentials from
    standard Google Cloud environment variables like GOOGLE_APPLICATION_CREDENTIALS
    if GOOGLE_API_KEY is not directly configured for the genai library.
    """

    def __init__(self, model_name: str = None, api_key: str = None):
        """
        Initializes the Gemini client.

        The client attempts to configure itself using environment variables if direct
        parameters are not provided.

        Args:
            model_name: The name of the Gemini model to use. Overrides the
                        TEXTURA_LLM_MODEL environment variable. If neither is set,
                        defaults to "gemini-1.5-pro-latest".
            api_key: The Google API key. Overrides the GOOGLE_API_KEY environment
                     variable. This is essential for API communication.
        """
        resolved_api_key = api_key or os.environ.get("GOOGLE_API_KEY")
        if not resolved_api_key:
            # In a real application, you might raise an error or log a strong warning.
            # For now, we'll print a warning as actual calls won't be made in the sandbox.
            print("Warning: GOOGLE_API_KEY not found in environment or passed directly.")
            # raise ValueError("API key must be provided either directly or via GOOGLE_API_KEY environment variable.")

        genai.configure(api_key=resolved_api_key)

        self.model_name = model_name or os.environ.get("TEXTURA_LLM_MODEL") or "gemini-1.5-pro-latest"

        try:
            self.model = genai.GenerativeModel(self.model_name)
            # print(f"GeminiClient initialized with model: {self.model_name}") # Debug print
        except Exception as e:
            # Handle cases where model initialization might fail (e.g., invalid model name, API key issue)
            # In a real app, more robust error handling/logging would be here.
            print(f"Warning: Could not initialize Gemini model '{self.model_name}'. Error: {e}")
            self.model = None # Ensure model is None if initialization fails

    def predict(self, prompt: str, **kwargs: Any) -> str:
        """
        Sends a prompt to the Gemini model and returns the generated text response.

        Args:
            prompt: The prompt string to send to the LLM.
            **kwargs: Additional keyword arguments for Gemini's `generate_content` method
                      (e.g., `generation_config`, `safety_settings`).

        Returns:
            The text response generated by the LLM.
            Returns an empty string if the model is not initialized or an error occurs.
        """
        if not self.model:
            print("Error: Gemini model is not initialized. Cannot make prediction.")
            return "Error: Model not initialized." # Or raise an exception

        try:
            # Ensure that only valid GenerationConfig keys are passed if provided in kwargs
            # For simplicity, this example assumes kwargs are directly compatible or empty.
            # A more robust implementation would filter/map kwargs to GenerationConfig.

            # Example: if 'temperature' is in kwargs, create a GenerationConfig
            # generation_config_params = {}
            # if 'temperature' in kwargs:
            #    generation_config_params['temperature'] = kwargs.pop('temperature')
            # if 'max_output_tokens' in kwargs:
            #    generation_config_params['max_output_tokens'] = kwargs.pop('max_output_tokens')
            # current_generation_config = genai.types.GenerationConfig(**generation_config_params) if generation_config_params else None

            response = self.model.generate_content(prompt, **kwargs) # Pass through other kwargs

            # Handle potential issues with the response structure if needed.
            # For Gemini, response.text should usually exist if successful.
            if response and response.parts:
                # Concatenate text from all parts, as some responses might be multi-part.
                return "".join(part.text for part in response.parts if hasattr(part, 'text') and part.text)
            elif hasattr(response, 'text') and response.text:
                 return response.text
            else:
                # This case might indicate an issue or an unexpected response structure.
                # Log the full response for debugging if possible.
                print(f"Warning: Gemini response structure unexpected or missing text. Response: {response}")
                return "Error: Unexpected response structure from LLM."

        except Exception as e:
            # Log the error appropriately
            print(f"Error during Gemini API call: {e}")
            # Depending on policy, either raise the exception or return an error message/empty string
            return f"Error during API call: {str(e)}"

# Example of how it might be used (for testing purposes, won't run in sandbox due to API calls)
if __name__ == '__main__':
    # This block will likely not execute successfully in a sandboxed environment
    # without actual API keys and network access.
    print("Attempting to initialize GeminiClient (requires GOOGLE_API_KEY and network)...")
    try:
        # To test this locally, set GOOGLE_API_KEY environment variable
        # and optionally TEXTURA_LLM_MODEL
        client = GeminiClient()
        if client.model:
            print(f"GeminiClient initialized with model: {client.model_name}")
            sample_prompt = "What is the capital of France?"
            print(f"Sending prompt: '{sample_prompt}'")
            response_text = client.predict(sample_prompt)
            print(f"Received response: {response_text}")
        else:
            print("GeminiClient model could not be initialized. Check API key and model name.")
    except Exception as e:
        print(f"An error occurred during GeminiClient example: {e}")
